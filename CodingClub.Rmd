---
title: "SMCL Coding Club"
output:
  html_notebook: default
  html_document:
    df_print: paged
  pdf_document: default
  word_document: default
---

```{r}
#
```

# Pre-amble

When people make reaches they use many signals to determine where their hand is. This includes efferent-based predictions of hand location as well as afferent proprioceptive signals. Based on context people will rely more on predictions, and efferent signals to determine where their hand is. Here we trained people in the same perturbation with different types of information about the rotation. We test their explicit strategies to gauge if these contexts had effect on learning and then compare active and passive hand localization which respectively do or do not include efferent information. We expect groups with more explicit adaptation to rely on predictions more in their hand localization.

## Setup

Before any other steps, we make sure the machine is ready to run the code for the project. We install dependencies, and download the data.

The R environment for the is project is recorded in the renv.lock file that is part of this project. The code for `renv` itself should have been activated when opening the .Rprofile associated with this R Studio project, since it's embedded in the project. This should allow you to restore the exact set of packages used for this project.

**This chunk does not run automatically because it can be very time consuming. Run it manually to restore the R environment:**

```{r eval=F}
renv::restore()
```

Some of the code uses functions from our lab's `Reach` package which is not on an official package server, but can be installed from GitHub, using `remotes`.

**We do that below, but this chunk does not run by default since it's not necssary. Run manually to install the package.**

```{r eval=F}
library('remotes')
ip <- installed.packages()
if ('Reach' %in% ip[,'Package']) {
  if (ip[which(ip[,'Package'] == 'Reach'),'Version'] < "2025.02.16") {
    remotes::install_github('thartbm/Reach')
  }
} else {
  remotes::install_github('thartbm/Reach')
}
```

Finally, we have written some custom functions for this project, that we want available as well:

```{r}
# download and handle data:
source('R/data.R')
# scripts for training reaches:
source('R/reaches.R')
```

### Download data

Up to now, we prepared the machine by getting code ready. The first bit of code we run downloads the data, which is perhaps also a step in getting the machine ready for useful stuff. The data is stored on this OSF repository: [SMCL coding club](https://osf.io/m5dt4/).

**The code below does not run by default, to reduce traffic to OSF.io. Run the chunk manually to download the data.**

```{r eval=F}
getData()
```

# Overview of experiment and data

In this experiment participants adapted to a 30 degree rotation, with various kinds of feedback:

1. Regular cursor feedback: "control"
2. Cursorjump feedback , where the rotation is shown at 1/3rd the reach on each training trial: "cursorjump"
3. A group where both the cursor and hand were visible during training: "handview"
4. A group that was explained the rotation beforehand: "instructed"

All reach were to targets at 45, 90 and 135 degrees.

## Data

There are several kind of data the we could analyse:

1. Training reaches
2. No-cursor reaches
3. Passive localization
4. Active localization

Both collected in the aligned and rotated phase of the task. With repetitions of each. See illustration below.

![Schedule of task order for all conditions.](doc/explicit_task_order.png){width=100%}


## Analysis plan

We want to know if the effect of efference copies on hand localization decreases when perturbations are more externally attributed (or perhaps when there is a larger explicit strategy).

### Training

Although not the main variable of interest, we do need to first check the training data:

- Did participants learn? Maybe we exclude some. If there is no learning, we do not get any hand localization shifts either, or at least we can't rely on them.
- Did all groups learn to the same extent? If they do, we can directly compare the size of other variables as well, if not, we'll have to rethink the rest of the analyses.
- Did all groups learn equally quickly? Perhaps this is not that important for our main questions, but it might be interesting to check.

### No-cursors

The manipulations might have an effect on implicit adaptation, here measured as without strategy no-cursor reach deviations. They should have an effect on explicit adaptation, which is only indirectly available in this project as the difference between with and without strategy no-cursor reach deviations.

- How do without-strategy no-cursor reach deviations vary with condition? If they do, there is an effect on implicit adaptation.
- Are there differences between with- and without strategy no-cursor reach deviations? Where we find any such difference, there is definitely a strategy.
- We will - for this example project - use the difference between the two kinds of no-cursor reach deviation as an estimate of explicit strategy, even though we do not really agree with this measure anymore. How does that look in each condition?

### Hand localization

Here we come to the main questions. We know that visuomotor adaptation also changes where we estimate the location of our hand within the workspace. We've shown before that there is a difference between active and passive hand localization. Active hand localization depends on predictions using an efference signal. These predictions may change depending on how we learn to adapt. For example, when adaptation more explicit or more externally attributed (or likely both, in the cursorjump and handview groups as compared to the control group). If hand localization is an implicit process, then the absence or presence of a strategy alone might not affect the predictions we (unconsciously) make, and active localization could still have increased hand localization shifts whether or not there is a strategy. However, external error attribution may (will) also occur implicitly if it is clear enough, along with explicit strategies. Nevertheless, in that case, predictions may not be based on efferent signals regardless of whether they were available.

To investigate all of this, we do several checks and tests. 

- Is there a difference between active and passive localization? At least in some of the groups there should be, e.g. in the control group.
- Is this different in the different conditions? Do some conditions (cursorjump, handview) not show a difference between active and passive hand localization shifts.
- Can this potentially be explained by different levels of explicit adaptation in the different conditions? Or potentially by external error attribution?

# Reach Training

## Reach deviations

In order to calculate a reach deviation, we can use the `atan2()` function, which takes a Y and X coordinate (note the order!), and returns the angle of a line going from the origin of the coordinate system (0,0) to the point (X,Y) in radians. The issue with this function is that its' output switches from $+\pi$ to $-\pi$ exactly to the left of the origin of the coordinate system. So a small change in position, gives almost the maximum change in direction. Especially if we have targets close to this switch point, then even with accurate reaches, performance could seem to be extremely bad. We need a solution to this.

In order to avoid this "switch-point" in the output, we rotate reach samples (either the whole trajectory, or single points on the reach, whatever we're working with) by -1 times the direction of the target. All reaches can then be seen as if the target were at 0Â°. And applying `atan2()` to such rotated trajectories, immediately gives the deviation from the target, so no further steps are necessary.

This rotation is most easily achieved by multiplying the trajectory samples with a [rotation matrix](https://en.wikipedia.org/wiki/Rotation_matrix) $R$. This is handled in the `getReachDeviation()` function, that's defined in the file `R\reaches.R`.

First we, need to convert the target angle (usually stored in degrees) to radians:

$$ \theta = \pi(-target/180) $$

Then we can use that to construct $R$:

$$ R = \begin{bmatrix}
cos(\theta) & -sin(\theta) \\
sin(\theta) & cos(\theta) \\
\end{bmatrix} $$

We get our set of trajectory samples, a vector $v$:

$$v = \begin{bmatrix}
x \\
y \\
\end{bmatrix} $$

And multiply it with the rotation matrix:

$$\bar{v} = Rv $$
This new vector $\bar{v}$ now contains the rotated trajectory samples, and we can apply `atan2()` to them.

> Note that $x$ and $y$ can be vectors, such that $v$ and $\bar{v}$ are not column matrices, but rectangular. This can be used to optimize the code for speed. You would first select a sample from each reach with the same target, add the coordinates to a matrix. Then multiply all of them with the same rotation matrix. This means that you only construct the rotation matrix for each target once, and all the matrix multiplication is done at the same time - with fewer loops, and less time spent in R interpretation. The code will be more complicated though, so we will not do this here.


1. Calculate baseline (trials 31 - 45: 5 reaches per target)

- use more trials to estimate baseline bias
- make function to remove outliers
- remove outliers from baseline reach deviations
- estimate baseline biases separately for each target

2. Correct rotated reaches for baseline deviations

3. Plot a learning curve


```{r fig.width=8, fig.height=6}
plotTraining()
```



4. Determine the asymptotic level of adaptation. And perhaps the rate of learning.

## Statistics

mixed design:
- "block" early 1-3, still 4-6, end: last 15
- group / condition

Let's do a mixed-design ANOVA on reach deviation, using block (trials: 1-3, 4-6, and 76-90) and group (control, cursorjump and handview) as factors.

```{r}
learningCurveANOVA()
```

As expected, there is a main effect of block (F(1.85,120.50)=83.78, p<.001). There is no effect of group nor an interaction.




Bayesian statistics:
- ANOVA, like above
- or F-test



We do a Bayesian analysis on the extent of learning (block 4 from above, trials 76-90):

```{r}
learningCurveBayes()
```

This means there is no evidence for or against an different extent of learning between the groups.

Exponential curve fitting:
- asymptote,
- rate of learning





Do we just take the average reach deviation at the end of the 90 trial block, or do we fit an exponential curve?

(5. Check if the top-up blocks are all the same as well?)



# Plot all figures

```{r}

for (target in c('pdf')) {

  plotTraining(target=target)

}

```

